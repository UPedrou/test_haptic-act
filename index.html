<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Haptic-ACT">
  <meta property="og:title" content="Haptic-ACT"/>
  <meta property="og:description" content="Haptic-ACT - Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers"/>
  <meta property="og:url" content="https://upedrou.github.io/test_haptic-act/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Haptic-ACT - Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers">
  <meta name="twitter:description" content="Haptic-ACT - Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Haptic-ACT</title>
  <link rel="icon" type="image/x-icon" href="static/images/haptic_act.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Haptic-ACT - Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=YW_5du0AAAAJ&hl" target="_blank">Pedro Miguel Uriguen Eljuri<sup>1*</sup>,</a></span>
              <span class="author-block">
                 Hironobu Shibata<sup>2</sup>,</span>
              <span class="author-block">
               Maeyama Katsuyoshi<sup>2</sup>,</span>
              <span class="author-block">
              Yuanyuan Jia<sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.co.jp/citations?user=dPOCLQEAAAAJ&hl=ja&oi=ao" target="_blank">Tadahiro Taniguchi<sup>1,2</sup></a></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <sup>1</sup>Kyoto University,
                <sup>2</sup>Ritsumeikan University,
                <br><span>Under Review</span>
              </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>
                    Paper (coming soon)
                  </span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                </a>
              </span>

                <!-- ArXiv abstract Link -->
<!--                 <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                    
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this paper we introduce Haptic-ACT, an advanced robotic system for pseudo oocyte manipulation, integrating multimodal information and Action Chunking with Transformers (ACT).
            Traditional automation methods for oocyte transfer rely heavily on visual perception, often requiring human supervision due to biological variability and environmental disturbances.
            Haptic-ACT enhances ACT by incorporating haptic feedback, enabling real-time grasp failure detection and adaptive correction.
            Additionally, we introduce a 3D-printed TPU soft gripper to facilitate delicate manipulations.
            Experimental results demonstrate that Haptic-ACT improves the task success rate, robustness, and adaptability compared to conventional ACT, particularly in dynamic environments.
            These findings highlight the potential of multimodal learning in robotics for biomedical automation.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Content of the paper -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Overview</h2>
        <div class="item">
          <div class="image-container">
            <img src="static/images/Overview_3.png" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div>
          <h2 class="subtitle">
            Haptic-ACT is a multimodal learning framework for robotic manipulation that enables delicate, contact-rich tasks by combining vision, robot proprioception, and haptic feedback.
            Our method extends Action Chunking with Transformers (ACT) to incorporate force sensing, allowing the robot not only to plan ahead but also to detect and recover from grasp failures autonomously.
            We apply Haptic-ACT to the task of pseudo-oocyte transfer, where the robot must grasp and place fragile biological objects safely.
            Unlike vision-only methods, our system can detect failed grasps in real time and retry, improving robustness significantly.
            Experiments in both trained and unseen environments show that Haptic-ACT improves manipulation success rates.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-4">Model Architecture</h2>
        <div class="item">
          <div class="image-container">
            <img src="static/images/ACT_Force_crop.png" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div>
          <h2 class="subtitle">
            The Haptic-ACT architecture extends the original ACT framework by integrating multimodal sensory inputs: Vision (camera images), Robot proprioception (joint angles, gripper position) and Haptic feedback (force sensor readings).
          </h2>
          <h2 class="subtitle">
          These three inputs are encoded into feature vectors and fused into a single multimodal representation. This fused input is passed to a Transformer-based model, which predicts a chunk of future actions.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-4">Data Collection</h2>
        <div class="item">
          <!-- <div class="image-container">
            <img src="static/images/ACT_Force_crop.png" alt="image of model" style="display: block; margin: 0 auto; width: 70%; height: auto;" />
          </div> -->
          <h2 class="subtitle">
            To train Haptic-ACT, we collected a diverse set of expert demonstrations by teleoperating the robot. To teleoperate the robot we are using a Touch-X haptic device.
          </h2>
          <h2 class="subtitle">
            In total, we collected 40 successfull demonstrations of grasping and placing the pseudo oocyte on a test tube and 10 recovery demonstrations, where the robot retries after a failed grasp.
          </h2>
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/robot_environment.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Hardware Environment.
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/end_effector.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Robot End Effector.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Introduction</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/tTRflwkiFno?si=4HspnWdo8zwfgrMP" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


    <!-- Video carousel -->
<!--     <section class="hero is-small">
      <div class="hero-body">
        <div class="container">
          <h2 class="title is-3">Another Carousel</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item item-video1">
              <video poster="" id="video1" autoplay controls muted loop height="100%"> -->
                <!-- Your video file here -->
<!--                 <source src="static/videos/carousel1.mp4"
                type="video/mp4">
              </video>
            </div>
            <div class="item item-video2">
              <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
                <!-- Your video file here -->
<!--                 <source src="static/videos/carousel2.mp4"
                type="video/mp4">
              </video>
            </div>
            <div class="item item-video3">
              <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
                <!-- Your video file here -->
<!--                 <source src="static/videos/carousel3.mp4"
                type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End video carousel -->







<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{uriguen2025hapticact,
        author={Uriguen Eljuri, Pedro Miguel and Shibata, Hironobu and Maeyama, Katsuyoshi and Jia, Yuanyuan and Taniguchi, Tadahiro},
        title={Haptic-ACT - Pseudo Oocyte Manipulation by a Robot Using Multimodal Information and Action Chunking with Transformers},
        year={2025, under review}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<!--Laboratory Information -->
<section class="section" id="Laboratory Information">
  <div class="container is-max-desktop content">
    <h2 class="title">Laboratory Information</h2>
    <ul>
      <li><a href="https://sites.google.com/view/tanichu-lab-ku/" target="_blank" rel="noopener noreferrer">Taniguchi Laboratory (Kyoto University)</a></li>
      <li><a href="http://www.em.ci.ritsumei.ac.jp/" target="_blank" rel="noopener noreferrer">Emergent Systems Laboratory (Ritsumeikan University)</a></li>
    </ul>
  </div>
</section>
<!--End Laboratory Information -->

  <!--Acknowledgements citation -->
  <section class="section" id="Acknowledgements">
    <div class="container is-max-desktop content">
      <!-- <h2 class="title">Acknowledgements</h2> -->
      <h2 class="title">Funding</h2>
      <p>
        This work was supported by the Japan Science and Technology Agency (JST) Moonshot Research & Development Program, Grant Number JPMJMS2033.
      </p>
    </div>
  </section>
  <!--End Acknowledgements citation -->


  
  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
